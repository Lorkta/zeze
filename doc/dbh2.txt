//////////////////////////////////////////////////////////////////////////////////////
// 旧内容，可快速略过。

dbh2
	或者叫dbh2，不依赖db.h，这个名字有纪念意义。
	rocksdb桶：部分表，部分key。
	桶的管理和分配。关键！
	桶的raft。
	并发：zeze专用，无需复杂并发控制。或者几乎不需要？
	使用c++？【第一版还是java，以后真有用处，并且找到人，再转成c++】
	raft c++？
	c++ net？wepoll

Database
	每个Database包含多张表。
	多Database。

Table
	kv表

Bucket
	每个kv表按一定规则分到不同的桶里面。
	每个桶由raft节点组成。

Bucket-Route
	kv划分规则：1. sort；2. hash

Bucket-Route-Sort
	需要一个有顺序按段划分byte[]的算法。关键问题是划分段多大。下面的方法能工作。
	现在考虑到的方法是动态规划路由：一开始一个桶，容量超过了，就从现有的记录数量中间拆分成两个。
	然后两个桶来用。等到其中一个桶又超出了，再拆分。暂不考虑合并桶（删除是少见的）。

Bucket-Route-Hash（第一版不实现）
	一致性hash算法。
	当桶增加时，需要能感知需要迁移的桶。
	现在Zeze采用的一个最常用的算法，可以继续采用，但是由于感知迁移桶的需求，
	不能使用虚拟节点。暂时考虑的策略是，一开始就建立足够数量桶（节点）。

Bucket-Hot-Split(Sort)
	热点桶二分法拆分，规则和Bucket-Route-Sort一样。

进程（模块）
	Master，知道所有数据库，所有表，所有桶的分布情况。提供表创建，桶信息查询。
	Dbh2Manager，桶运行容器，包含多个桶。
	Dbh2Agent，嵌入到dbh2的客户端内执行。由于嵌入执行，App会影响一点提交安全性。见CommitServer。
	CommitServer，Dbh2Agent的提交功能移到这里，独立服务器，仅用来提交事务。
	* Dbh2Agent/CommitServer 二选一，可配置。

===============================================================
问题
===============================================================
1. 两段式事务。
根本支持来自RocksDb；下面的逻辑在Agent中执行。
var transList = new list(); // 维护当前事务需要访问的所有Database的事务实例。
foreach (var key in keys)
{
	var dbInfo = Master.Query(key); // 为了性能，需要cache，问题？cache更新以及最终用目标Database主动报错。
	var conn = Dababase.getOrAdd(out maybe transList, dbInfo); // 查询或新建连接，第一次建立链接需要创建事务。
	conn.executeUpdate(key, value);
}
foreach (var trans in transList)
{
	trans.commit();
}

2. 事务并发。
上面的事务是多记录写，读取总是一个事务读取一条记录。对于读写并发，RocksDb已经保证了基本的并发。
其他多记录写的事务并发由Zeze解决。Zeze保证所有的写事务之间没有交集。
所以！本系统不需要任何并发控制。【桶的迁移是一个需要锁并且等待的操作，这个待定】

3.Raft怎么接入以及完整的事务方案
     RocksDB+Batch
    【确定采用这个方案了，原来麻雀版的基于OptimisticTransactionDB的方案作废】
     同一个桶的数据操作合并到一个批处理中，一次完成。

    . Zeze用Dbh2.Agent 对任意表任意记录进行flush时，Agent负责把所有的flush分组到不同桶的batch里面。分解完成，事务提交时，才开始和桶交互。
    . 分为PrepareBatch,CommitBatch,UndoBatch。
    a) PrepareBatch 桶的处理是首先检查数据确实在这个桶内，并收集这个Batch的UndoLog（这里说法不对，详见后面的实现细节），并加锁，
    然后Raft.appendLog进行apply。关于加锁，数据访问并发实际上由zeze保证互斥，这个加锁仅仅
    用于桶的动态并发拆分。有点大材小用的感觉。但是要支持动态并发拆分好像也没有其他好办法。
    b) CommitBatch，桶释放锁，删除内存中的Batch状态。
    c) UndoBatch，桶把UndoLog作为一次Raft.appendLog请求重做一次。

【Agent 伪码】
foreach(bucket : buckets)
	bucket.PrepareBatch(); // 任何Prepare失败，执行全部UndoBatch(); 这里略。
localTransactionFileLog.add(tid，buckets, timestamp); // 二段式提交关键点。持久化以后用于系统宕机重启以后继续提交。
foreach (bucket : buckets)
	bucket.CommitBatch();
localTransactionFileLog.remove(tid); 【不能删除，慢速GC，删除7天前的】
see https://blog.csdn.net/lengxiao1993/article/details/88290514

【桶的实现伪码】
Raft.Leader.OnPrepareBatch(batch) {
	// 由于锁定解锁跨线程，所以这里的锁应该用信号量(Semaphore)。
	if (inBucketAndLock()) {
		// leader需要检查inBucket和lock error，并马上报告结果。
		tid = allocateTid();
		appendLog(tid, batch);
		r.Result.setTid(tid);
		r.sendResult();
		return;
	}
	r.sendResultCode(error_code);
}
LogPrepareBatch.apply(tid, batch) {
	// leader在预先检查阶段已经加锁，事务会存在。所以用getOrAdd。
	// 构造函数里面加锁。
	var txn = transaction.getOrAdd(tid, new Dbh2Transaction(batch));
	batchTransaction.put(tid, txn);
}

Raft.Leader.OnCommitBatch(tid) {
	appendLog(tid);
}
LogCommitBatch.apply(tid) {
	// 所有raft节点。
	var txn = batchTransaction.remove(tid);
	if (null != txn) // 允许重复的Commit。
		txn.close(); // unlockAll
}

Raft.Leader.OnUndoBatch(tid) {
	appendLog(tid);
}
LogUndoBatch.apply(tid) {
	// 所有raft节点。
	var txn = batchTransaction.remove(tid);
	if (null != txn) // 允许重复的RaftUndoLog.
		txn.undoBatch(bucket);
}

【问题】
1). commit流程不能失败，会不停尝试。关键点见【Agent伪码的localTransactionFileLog.add】。
2). 事务系列化的能力。
    没有完成的事务，桶会保持锁，所以后续的读写都会失败，这样虽然保证了事务系列化，
    但是系统可用性被agent所在服务器波及到。
3). commit关键点是否独立出新的服务进程。
    由于现在commit由agent控制，agent嵌入zeze-server进程，而zeze-server进程比较复杂，
    又没有可靠性支持（如raft），本来是无状态设计。如果agent所在zeze-server一直没有恢复，
    由于第2点，会造成系统慢慢变得不可用。可以考虑采用把commit流程独立成新的服务，
    这个新的服务更纯粹，更不容易出错，另外这个新的服务可以加入raft，提高可用性。
4). UndoBatch 可重复，桶忽略重复的。
5). CommitBatch 可重复，桶忽略重复的。
6). Raft-Snapshot 截断相关操作。
     PrepareBatch 创建了logs，进行中的操作才可以Undo（Undo的数据来源）。
     如果Snapshot把未完成的事务的PrepareBatch截断了，Undo将无法进行。
     解决方法：raft支持延迟snapshot，本次snapshot时，实际提交的是上一次（如果存在）的snapshot。
7). PrepareBatch 流程中断造成桶的正在进行的事务一直空悬。
     桶主动查询事务所在的Agent，判断是否（localTransactionFileLog）关键点(isCommtting)，
     if (isCommitting) commit(); else undo(); 查询失败下一次尝试，必须能查询到结果。
     【localTransactionFileLog 在事务成功结束以后不会删除】。

4. 桶按key排序分段，写入是怎么提高并发。
    hash方式看起来具有极大的并发写入速度。

5.并发walk，同时向不同的DatabaseService发送walk请求。归并（按顺序？）

6.MapReduce。

7. 分桶需求
	分桶时仍然能提供服务。
	Master维护所有的桶信息，分桶完成之前，不会route的目标桶。
	Master拥有正在进行分桶的信息，目前看来，仅仅用来监控。

8. 并发分桶
分桶阶段一（同步数据阶段），
a)Dbh2Manager找到分桶的中间key。
b)Dbh2Manager通过Master查找可以创建新桶的manager并创建分桶，此新创建的桶不可见。
c)Dbh2Manager开始拷贝中间key之后（是否包含？）数据到新桶。维护已经拷贝过去的current-key。
d)Dbh2Manager此时继续提供服务，如果事务更新的数据在key,current-key之间，这些数据操作马上同步到新桶。
e)Dbh2Manager当同步拷贝完成，新桶写入Meta，更新Master信息，新桶此时已经开始可以接受服务。旧桶写入新的Meta，回退key之后的事务！。
   【讨论】上面的操作最好能原子执行，但涉及多个进程，一般意义上的原子保证可能比较难，是否可以通过执行顺序就保证正确性。
   【修订】rocksdb查询得到table的大概key的数量。上面的方法是从最后一个记录倒序遍历key-count/2条记录。
                由于倒序遍历效率比正序慢，所以拷贝前一半到新的分桶效率更高。上面的描述需要修订。
   【断点续传】
                分桶过程中，记录最后复制的key用于续传。
                分桶过程中，发生[first,current-key]之间的事务操作马上同步到目标桶。

分桶阶段二（回退阶段），
a)Dbh2Manager发现batch中不是本桶的记录，收集以后回退给agent，并报告桶信息需要更新错误码。
b)Agent收到回退，创建新的Batch，迁移记录到此。
c)Agent更新桶信息，访问新桶，发送新创建的Batch给它。

分桶阶段三（删除分走的数据）
此阶段和阶段二并行。

9. 分桶Detail
开始实现分桶以后还是碰到了一些问题，跟张路讨论了以后，重新整理一下，分几个方案。
a) 【原思路】

kv.record.lock 事务进行中会锁住，悬挂的时候阻止后续的写，符合zeze事务系列化要求。最终事务系列化安全保证。

i. 复制流程
var it = data.iterator(tailing); // tailing 表示这个itertor遍历的时候能看到后面的新加入的记录。
var count = rocks.getKeyNumber() / 2;
it.seekTo(share.saved.currentKey); // 第一次拷贝需要seekToFirst()；断点续传时从上次拷贝完成的记录之后开始。
count -= saved.copyed.count;
for (int i = 0; i < count && it.isValid(); ++i) {
	var key = it.key();
	var lock = Locks.get(key);
	lock.lock();
	try {
		copy to new split bucket;
		share.saved.currentKey = it.key();
		saved.copyed.count = i + 1; // 先不考虑效率，每个记录保存一次。
	} finally {
		lock.unlock();
	}
}
ii. 事务同步流程
// lock
onPrepareBatch() {
	// 这里会lock住所有的记录。如果记录正在拷贝（see i. 复制流程中的lock），会同步等待一下。
}

// under lock
onCommitBatch() {
	var sync = new list();
	for (var r : updatedRecordsInBatch)
		if (r.key <= share.saved.currentKey)
			sync.add(r);
	sync to new split bucket;
	unlockAll();
}

b) 【新思路】

几乎无锁，kv.record.lock 还是用来保护zeze事务系列化，但是和分桶复制无关了。

i. kv表删除的记录重新定义
记录删除的时候，不会真正删除，而是会留下一个特殊值的记录。

ii. 事务同步流程
如果分桶同步中，所有的将要分桶的记录都同步。所以这个方案需要先确定分桶的last记录。
注意，上面的原思路只需要count，last可以在最后知道。这个last使用见下面。
share.last = calculateSplitLastKey();

onCommitBatch() {
	var sync = new list();
	for (var r : updatedRecordsInBatch)
		if (r.key <= share.last)
			sync.add(r);
	sync to new split bucket;
	unlockAll(); // 这里仅仅保护进行中的事务不能并发。和复制流程无关。
}

iii. 复制流程
for (var first = getFirst(); first != share.last; ++first) {
	copy first to new split bucket;
}

onCopySplitRecord(key, value) {
	rocksdb.putIfAbsent(key, value); // 【关键点】putIfAbsent, 复制流程级别低，如果存在事务同步过来的结果，copy的丢弃。
}

iv. 总结
【kv.deleted.record 特殊值留存】
【copy.putIfAbsent】

c) a-b 差异总结
i. 存储差异，deleted.record是否真的删除。
ii. 复制流程对last的定位方式不同，b方案需要先遍历一次。
iii. 复制流程当复制记录时是否加锁。
iv. 事务同步范围不同，a方案范围逐步变大，b方案总是同步。
